{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace, Sequence, ByteLevel\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = ByteLevel()\n",
    "tokenizer.decoder = ByteLevelDecoder()\n",
    "\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[START]\", \"[END]\", \"[PAD]\"], vocab_size=16384)\n",
    "\n",
    "files = [\"corpus150.txt\"]\n",
    "tokenizer.train(files, trainer)\n",
    "\n",
    "#tokenizer.encode('[START] A duck is a carnivorous animal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset file of size 1537774\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import codecs\n",
    "from timeit import timeit\n",
    "\n",
    "class Data:\n",
    "    def __init__(self):\n",
    "        filename = 'corpus150.txt'\n",
    "        \n",
    "        self.file = codecs.open(filename, 'r', encoding='utf-8', errors='ignore')\n",
    "        self.file_length = os.stat(filename).st_size\n",
    "        \n",
    "        print('Loaded dataset file of size', self.file_length)\n",
    "        \n",
    "    def sample_batch(self, n=32, length=240):\n",
    "        # sample a lot of strings of certain length\n",
    "        strs = []\n",
    "        for i in range(n):\n",
    "            self.file.seek(random.randrange(0, self.file_length - length))\n",
    "            strs.append(self.file.read(length))\n",
    "        \n",
    "        # encode with tokenizer\n",
    "        x = [encoding.ids for encoding in tokenizer.encode_batch(strs)]\n",
    "        \n",
    "        # shorten the long ones\n",
    "        min_len = min(map(len, x))\n",
    "        x = [ids[0:min_len] for ids in x]\n",
    "        \n",
    "        # put it into pytorch preferred format (torch.tensor, with shape (sequence, batch))\n",
    "        x = torch.tensor(x)\n",
    "        x = x.transpose(1, 0)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "dataset = Data()\n",
    "\n",
    "#timeit(dataset.sample_batch, number=100) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(torch.rand((2,2)) > 0.8).float() * torch.ones()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 1., 0., 1., 0., 1., 0.])\n",
      "tensor([0., 1., 0., 1., 0., 1., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "def checker_board(d_model):\n",
    "    half = (d_model) // 2\n",
    "    texture = torch.cat([\n",
    "        torch.ones((half, 1)),\n",
    "        torch.zeros((half, 1))\n",
    "    ], dim=1).view((-1,))\n",
    "    \n",
    "    return texture\n",
    "\n",
    "print(checker_board(8))\n",
    "print(-checker_board(8) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_embedding(x):\n",
    "        # x: (pos, n, i)\n",
    "        \n",
    "        length = x.shape[0]\n",
    "        batch_size = x.shape[1]\n",
    "        d_model = x.shape[2]\n",
    "\n",
    "        i = torch.arange(0, d_model).view((1, 1, -1)).expand(length, -1, d_model).to(device).float()\n",
    "        pos = torch.arange(0, length).view((-1, 1, 1)).expand(length, -1, d_model).to(device).float()\n",
    "        \n",
    "        z = pos / 10000 ** (i / d_model)\n",
    "        \n",
    "        sin = torch.sin(z)\n",
    "        cos = torch.cos(z)\n",
    "        \n",
    "        sin_mask = checker_board(d_model).to(device)\n",
    "        cos_mask = -sin_mask + 1\n",
    "                \n",
    "        pe = (sin_mask * sin) + (cos_mask * cos)\n",
    "        pe = pe.expand(length, batch_size, d_model)\n",
    "        \n",
    "        return x + pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, dropout=0.1, embedding_dim=512, heads=8, num_layers=6):\n",
    "        super(Model, self).__init__()\n",
    "        # config\n",
    "        self.dropout = dropout\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.heads = heads\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.start_token = torch.tensor([[1]]).to(device)\n",
    "        \n",
    "        # layers\n",
    "        self.embedding = nn.Embedding(num_embeddings=16384, embedding_dim=embedding_dim)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(embedding_dim, heads, dim_feedforward=2048, dropout=dropout)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(embedding_dim, heads, dim_feedforward=2048, dropout=dropout)\n",
    "        \n",
    "        self.encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.decoder = torch.nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.unembedding = nn.Linear(512, 16384)\n",
    "        self.unembedding.weight.data = self.embedding.weight.data\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        source = pos_embedding(x)\n",
    "        target = pos_embedding(torch.cat([\n",
    "            self.embedding(self.start_token).expand(1, source.shape[1], -1),\n",
    "            x\n",
    "        ], dim=0)[:-1])\n",
    "        \n",
    "        source_mask = (torch.rand((source.shape[0], source.shape[1], 1)) > 0.1).float().expand(-1, -1, self.embedding_dim).to(device)\n",
    "        source = source * source_mask\n",
    "        \n",
    "        target_mask = torch.triu(torch.full((target.shape[0], target.shape[0]), float('-inf')), diagonal=1).to(device)\n",
    "        \n",
    "        memory = self.encoder(source)\n",
    "        output = self.decoder(target, memory, tgt_mask=target_mask)\n",
    "        \n",
    "        output = self.unembedding(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def use(self, source, duration=20):\n",
    "        source = pos_embedding(self.embedding(source))\n",
    "        memory = self.encoder(source)\n",
    "        \n",
    "        target = [1]\n",
    "        \n",
    "        for i in range(duration):\n",
    "            target_embedded = pos_embedding(self.embedding(torch.tensor(target, dtype=torch.long).view((-1, 1)).to(device)))\n",
    "            output = self.unembedding(self.decoder(target_embedded, memory))[-1, 0]\n",
    "            output = torch.distributions.categorical.Categorical(probs=F.softmax(output, dim=0)).sample()\n",
    "            #print(output_dist)\n",
    "            target.append(output.item())\n",
    "        \n",
    "        return target\n",
    "        \n",
    "    def beam(self, source, k=15, length=30):\n",
    "        # encode\n",
    "        source = self.embedding(source)\n",
    "        source = pos_embedding(source)\n",
    "        memory = self.encoder(source)\n",
    "        # copy memory for each k we'll be processing\n",
    "        memory = memory.expand(-1, k, -1)\n",
    "        \n",
    "        # keep [k] active branches of [length]\n",
    "        target = torch.zeros((length, k), dtype=torch.long).to(device)\n",
    "        scores = torch.zeros((length, k)).to(device)\n",
    "        # every branches root is a start token\n",
    "        target[0, :] = 1\n",
    "        scores[0, :] = 0\n",
    "        \n",
    "        # decode\n",
    "        for i in range(1, length):\n",
    "            # make predictions for each current branch\n",
    "            target_embedding = pos_embedding(self.embedding(target[:i]))\n",
    "            output = self.unembedding(self.decoder(target_embedding, memory))[-1:]\n",
    "            \n",
    "            # find [k] best branches from each current branch.\n",
    "            y = torch.topk(output, k=k, dim=2)\n",
    "            \n",
    "            # find the value of current branches as they are\n",
    "            current_evidence = scores[:i].sum(dim=0).view((1, k)).unsqueeze(2).expand(1, -1, k)\n",
    "            \n",
    "            # add that to the value of each possible branch for each current branch\n",
    "            branch_evidence = current_evidence + y.values\n",
    "            \n",
    "            # decide which possibilities should be leaf of the new [k] current branches based on the highest total value\n",
    "            y_topk = torch.topk(branch_evidence.view((-1)), k=k)\n",
    "            \n",
    "            # find out which current root branch the new leafs belong to\n",
    "            current_k_root = y_topk.indices // k\n",
    "            \n",
    "            # replace target & scores with the best leafs, preceded by their current branch\n",
    "            new_target = torch.cat(\n",
    "                [\n",
    "                    target[0:i, current_k_root], # current branch that the leaf comes from\n",
    "                    y.indices.view((-1,))[y_topk.indices].view((1, k)) # leaf\n",
    "                ],\n",
    "                dim=0\n",
    "            )\n",
    "            new_scores = torch.cat(\n",
    "                [\n",
    "                    scores[0:i, current_k_root],\n",
    "                    y.values.view((-1,))[y_topk.indices].view((1, k))\n",
    "                ],\n",
    "                dim=0\n",
    "            )\n",
    "            \n",
    "            target[:i+1] = new_target\n",
    "            scores[:i+1] = new_scores\n",
    "            \n",
    "        return target[:, scores.sum(dim=0).argmax()]\n",
    "            \n",
    "        \n",
    "        \n",
    "model = Model().to(device)\n",
    "#model.load_state_dict(torch.load('state_dicts/bert6'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr(i, warmup_steps, model_params):\n",
    "    i += 0.001\n",
    "    base = model_params ** -0.5\n",
    "    \n",
    "    warmup = i * (warmup_steps ** -1.5)\n",
    "    general = i ** -0.5\n",
    "    \n",
    "    return base * np.minimum(warmup, general)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1, weight_decay=0.1)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda i: lr(i, warmup_steps=4000, model_params=512))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0006208584240704868\n",
      "loss tensor(92.8548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      " lub plays in the English Premier League and is owned by a company from Dubai. As of September 1, 2008 they are the richest club in the English Premier League.\n",
      "The manager of the club is the Welshman Mark Hughes who replaced Sven-Göran Eriksson. They finished 9th place in the 2007-08 Premier League\n",
      "================================\n",
      " exists90igsigs pubimchiigs Premierigsigs Branimchi Jainigs90imchi Jain90igs Advertigsigs domesticated Branimchi90909090 Bran909090 Premierlanca9090 Bran909090imchiigs Bran9090igs90 Bran Bran90lanca Bran Bran9090igslast Bran9090igs90imchi Branigs90 Branigs 20079090 Premier\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "#torch.cuda.empty_cache()\n",
    "for i in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "    x = dataset.sample_batch(n=32, length=400).to(device)\n",
    "    \n",
    "    y = model.forward(x)\n",
    "    \n",
    "    #print(y[1,0])\n",
    "    \n",
    "    loss = nn.CrossEntropyLoss()(y.view((-1, 16384)), x.reshape((-1)))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(optimizer.param_groups[0]['lr'])\n",
    "    print('loss', loss)    \n",
    "    #print(x.shape, y.shape)\n",
    "    #print(x[0:5, 0], y[0:5, 0].argmax(dim=1))\n",
    "    print(tokenizer.decode(x[:, 0].tolist()))\n",
    "    print('================================')\n",
    "    print(tokenizer.decode(y[:, 0].argmax(dim=1).tolist()))\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    scheduler.step()\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'state_dicts/bert6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. of of\\n. is.... by., of of\\n is is.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "tokenizer.decode(\n",
    "    model.beam(\n",
    "        torch.tensor(tokenizer.encode('England is a [UNK] country').ids, dtype=torch.long).view((-1, 1)).to(device),\n",
    "        k=15,\n",
    "        length=20\n",
    "    ).tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
