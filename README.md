# bert-style-transformer
Not actually BERT, just loosely inspired.

Trained slow, but it definitely learned something. That's perfect, because I just needed a grasp of how to use PyTorch transformers. 

I built beam search, positional encoding, a simple batch sampler, and training loop myself. For transformers and tokenization I used PyTorch and Huggingface respectively.
