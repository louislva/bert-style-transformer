{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace, Sequence, ByteLevel\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = ByteLevel()\n",
    "tokenizer.decoder = ByteLevelDecoder()\n",
    "\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[START]\", \"[END]\", \"[PAD]\"], vocab_size=16384)\n",
    "\n",
    "files = [\"corpus150.txt\"]\n",
    "tokenizer.train(files, trainer)\n",
    "\n",
    "#tokenizer.encode('[START] A duck is a carnivorous animal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset file of size 1537774\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import codecs\n",
    "from timeit import timeit\n",
    "\n",
    "class Data:\n",
    "    def __init__(self):\n",
    "        filename = 'corpus150.txt'\n",
    "        \n",
    "        self.file = codecs.open(filename, 'r', encoding='utf-8', errors='ignore')\n",
    "        self.file_length = os.stat(filename).st_size\n",
    "        \n",
    "        print('Loaded dataset file of size', self.file_length)\n",
    "        \n",
    "    def sample_batch(self, n=32, length=240):\n",
    "        # sample a lot of strings of certain length\n",
    "        strs = []\n",
    "        for i in range(n):\n",
    "            self.file.seek(random.randrange(0, self.file_length - length))\n",
    "            strs.append(self.file.read(length))\n",
    "        \n",
    "        # encode with tokenizer\n",
    "        x = [encoding.ids for encoding in tokenizer.encode_batch(strs)]\n",
    "        \n",
    "        # shorten the long ones\n",
    "        min_len = min(map(len, x))\n",
    "        x = [ids[0:min_len] for ids in x]\n",
    "        \n",
    "        # put it into pytorch preferred format (torch.tensor, with shape (sequence, batch))\n",
    "        x = torch.tensor(x)\n",
    "        x = x.transpose(1, 0)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "dataset = Data()\n",
    "\n",
    "#timeit(dataset.sample_batch, number=100) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(torch.rand((2,2)) > 0.8).float() * torch.ones()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 1., 0., 1., 0., 1., 0.])\n",
      "tensor([0., 1., 0., 1., 0., 1., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "def checker_board(d_model):\n",
    "    half = (d_model) // 2\n",
    "    texture = torch.cat([\n",
    "        torch.ones((half, 1)),\n",
    "        torch.zeros((half, 1))\n",
    "    ], dim=1).view((-1,))\n",
    "    \n",
    "    return texture\n",
    "\n",
    "print(checker_board(8))\n",
    "print(-checker_board(8) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_embedding(x):\n",
    "        # x: (pos, n, i)\n",
    "        \n",
    "        length = x.shape[0]\n",
    "        batch_size = x.shape[1]\n",
    "        d_model = x.shape[2]\n",
    "\n",
    "        i = torch.arange(0, d_model).view((1, 1, -1)).expand(length, -1, d_model).to(device).float()\n",
    "        pos = torch.arange(0, length).view((-1, 1, 1)).expand(length, -1, d_model).to(device).float()\n",
    "        \n",
    "        z = pos / 10000 ** (i / d_model)\n",
    "        \n",
    "        sin = torch.sin(z)\n",
    "        cos = torch.cos(z)\n",
    "        \n",
    "        sin_mask = checker_board(d_model).to(device)\n",
    "        cos_mask = -sin_mask + 1\n",
    "                \n",
    "        pe = (sin_mask * sin) + (cos_mask * cos)\n",
    "        pe = pe.expand(length, batch_size, d_model)\n",
    "        \n",
    "        return x + pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, dropout=0.1, embedding_dim=512, heads=8, num_layers=3):\n",
    "        super(Model, self).__init__()\n",
    "        # config\n",
    "        self.dropout = dropout\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.heads = heads\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.start_token = torch.tensor([[1]]).to(device)\n",
    "        \n",
    "        # layers\n",
    "        self.embedding = nn.Embedding(num_embeddings=16384, embedding_dim=embedding_dim)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(embedding_dim, heads, dim_feedforward=2048, dropout=dropout)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(embedding_dim, heads, dim_feedforward=2048, dropout=dropout)\n",
    "        \n",
    "        self.encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=3)\n",
    "        self.decoder = torch.nn.TransformerDecoder(decoder_layer, num_layers=3)\n",
    "        \n",
    "        self.unembedding = nn.Linear(512, 16384)\n",
    "        self.unembedding.weight.data = self.embedding.weight.data\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        source = pos_embedding(x)\n",
    "        target = pos_embedding(torch.cat([\n",
    "            self.embedding(self.start_token).expand(1, source.shape[1], -1),\n",
    "            x\n",
    "        ], dim=0)[:-1])\n",
    "        \n",
    "        source_mask = (torch.rand((source.shape[0], source.shape[1], 1)) > 0.1).float().expand(-1, -1, self.embedding_dim).to(device)\n",
    "        source = source * source_mask\n",
    "        \n",
    "        memory = self.encoder(source)\n",
    "        output = self.decoder(target, memory)\n",
    "        \n",
    "        output = self.unembedding(output)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "model = Model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.9766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      " hells) from the island. Ordnance is still buried or lying on the ground. Other items have washed down gullies and still other unexploded ordnance lies beneath the waters offshore. In 1981, the entire island was included on the National Register of Historic Places.\n",
      "\n",
      "================================\n",
      " hellsls from the island. Or Orance still still buried or lying on the ground. Other items have needs down burn and and still unexploded unexploded ordnance lies b bath the waters offshore. In 1981, the entire was was included on the National Reg hundreds of little scientists..\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0006)\n",
    "torch.cuda.empty_cache()\n",
    "for i in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    x = dataset.sample_batch(n=64, length=320).to(device)\n",
    "    \n",
    "    y = model.forward(x)\n",
    "    \n",
    "    #print(y[1,0])\n",
    "    \n",
    "    loss = nn.CrossEntropyLoss()(y.view((-1, 16384)), x.reshape((-1)))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('loss', loss)    \n",
    "    #print(x.shape, y.shape)\n",
    "    #print(x[0:5, 0], y[0:5, 0].argmax(dim=1))\n",
    "    print(tokenizer.decode(x[:, 0].tolist()))\n",
    "    print('================================')\n",
    "    print(tokenizer.decode(y[:, 0].argmax(dim=1).tolist()))\n",
    "    clear_output(wait=True)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
